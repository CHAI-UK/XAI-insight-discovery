{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxMmbory8p8y"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-survival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHY29qJzvUYX"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "from scipy.stats import gaussian_kde, binomtest\n",
        "\n",
        "from sksurv.datasets import load_gbsg2, load_whas500, load_aids\n",
        "from sksurv.datasets import get_x_y\n",
        "from sksurv.ensemble import RandomSurvivalForest\n",
        "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
        "from sksurv.preprocessing import OneHotEncoder\n",
        "from sksurv.metrics import concordance_index_censored, cumulative_dynamic_auc\n",
        "from sksurv.nonparametric import kaplan_meier_estimator\n",
        "\n",
        "from scipy.stats import mannwhitneyu, pearsonr\n",
        "from sklearn.utils import resample\n",
        "from patsy import dmatrix\n",
        "\n",
        "import shap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALrI_xal81P4"
      },
      "outputs": [],
      "source": [
        "# Split the dataset and fit the naive model and the recommender.\n",
        "def split_and_train(X, y, model_name, seed=20):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
        "\n",
        "    model = get_model(model_name, seed)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return model, (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "def get_model(name, seed=20):\n",
        "  if name == 'rf':\n",
        "    model = RandomSurvivalForest(n_estimators=500, min_samples_split=10, min_samples_leaf=15, n_jobs=-1, random_state=seed)\n",
        "  elif name == 'cox':\n",
        "    model = CoxPHSurvivalAnalysis()\n",
        "  else:\n",
        "    raise ValueError(f\"Unrecognised model name '{name}'!\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM5pLkzXi4tP"
      },
      "source": [
        "# SHAP Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVcPkODRvybi"
      },
      "outputs": [],
      "source": [
        "## Generate shap analysis results and shap plots for the Recommendar\n",
        "def _extract_time_event(y):\n",
        "        \"\"\"\n",
        "        Return (time, event) as 1D numpy arrays.\n",
        "\n",
        "        Supports:\n",
        "          * sksurv structured arrays from load_gbsg2, load_whas500, load_aids, ...\n",
        "          * pandas DataFrame with usual column names.\n",
        "        \"\"\"\n",
        "\n",
        "        # Case 1: structured array (what sksurv datasets return)\n",
        "        if isinstance(y, np.ndarray) and y.dtype.names is not None:\n",
        "            names = list(y.dtype.names)\n",
        "            if len(names) != 2:\n",
        "                raise ValueError(\n",
        "                    f\"Structured array y must have exactly 2 fields, got {names}\"\n",
        "                )\n",
        "\n",
        "            # By sksurv convention: first field = event indicator (bool),\n",
        "            # second field = time (float). :contentReference[oaicite:1]{index=1}\n",
        "            event = y[names[0]].astype(bool)\n",
        "            time = y[names[1]].astype(float)\n",
        "            return time, event\n",
        "\n",
        "        # Case 2: pandas DataFrame\n",
        "        if isinstance(y, pd.DataFrame):\n",
        "            # possible event column names across datasets\n",
        "            event_candidates = [\n",
        "                \"event\", \"Status\", \"status\", \"death\", \"fstat\",\n",
        "                \"cens\", \"censor\", \"censor_d\"\n",
        "            ]\n",
        "            time_candidates = [\n",
        "                \"time\", \"Survival_in_days\", \"lenfol\", \"time_d\", \"futime\"\n",
        "            ]\n",
        "\n",
        "            event_col = next((c for c in event_candidates if c in y.columns), None)\n",
        "            time_col = next((c for c in time_candidates if c in y.columns), None)\n",
        "\n",
        "            if event_col is None or time_col is None:\n",
        "                raise ValueError(\n",
        "                    \"Could not infer event/time columns in DataFrame y; \"\n",
        "                    \"got columns: \" + \", \".join(y.columns)\n",
        "                )\n",
        "\n",
        "            event = y[event_col].astype(bool).to_numpy()\n",
        "            time = y[time_col].astype(float).to_numpy()\n",
        "            return time, event\n",
        "\n",
        "        raise TypeError(\n",
        "            \"Unsupported type for y. Use a sksurv structured array or a pandas DataFrame.\"\n",
        "        )\n",
        "\n",
        "def get_explanations(model, X_test, y_test, eps, risk_level='low'):\n",
        "    names = list(y_test.dtype.names)\n",
        "    if risk_level == 'high':\n",
        "        risk_data = X_test[y_test[names[0]] == 1]\n",
        "    elif risk_level == 'low':\n",
        "        risk_data = X_test[y_test[names[0]] == 0]\n",
        "    else:\n",
        "        risk_data = X_test\n",
        "\n",
        "    # compute the reference point (average patient)\n",
        "    X_mean = risk_data.mean().to_frame().T\n",
        "    # find average risk\n",
        "    y_mean = model.predict(X_mean)\n",
        "\n",
        "    if (risk_level == 'high') | (risk_level == 'low'):\n",
        "      y_std = np.std(model.predict(risk_data))\n",
        "      # predict risk for all\n",
        "      y_pred = model.predict(X_test)\n",
        "\n",
        "      eps = y_std * eps\n",
        "\n",
        "\n",
        "      # select only those individuals whose change in risk with respect to the reference point is smaller than epsilon (i.e. close to 0)\n",
        "      sel_mask = np.abs(y_pred - y_mean) < eps\n",
        "\n",
        "      # use the reference point\n",
        "\n",
        "      sel_data = X_test[sel_mask]\n",
        "    else:\n",
        "      sel_data = X_test\n",
        "\n",
        "    ex = shap.KernelExplainer(model.predict, X_mean)\n",
        "    explanation = ex(sel_data)\n",
        "    df_shap = pd.DataFrame(explanation.values, columns=X_test.columns)\n",
        "\n",
        "    # get SHAP values of the selected individuals (delta_R close to 0)\n",
        "    return df_shap, sel_data\n",
        "\n",
        "def make_plot(df_shap, org_df, filename, plot_type='scatter', xlabel='SHAP value', figsize=(4, 3)):\n",
        "  features = df_shap.columns.to_list()\n",
        "  n_feats = len(features)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "  if plot_type == 'violin':\n",
        "    if org_df is not None:\n",
        "      shap_long = df_shap.melt(var_name='Feature', value_name='SHAP Value')\n",
        "      feat_long = org_df.melt(var_name='Feature', value_name='Feature Value')\n",
        "      data = pd.concat([shap_long, feat_long['Feature Value']], axis=1)\n",
        "    else:\n",
        "      raise ValueError('No original faeture values provided')\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "      df_feat = data[data['Feature'] == feature]\n",
        "      try:\n",
        "        kde = gaussian_kde(df_feat['SHAP Value'].values)\n",
        "        x_vals = df_feat['SHAP Value'].values\n",
        "        density_vals = kde(x_vals)\n",
        "\n",
        "        density_vals = density_vals/density_vals.max()*0.4\n",
        "\n",
        "        y_center = i\n",
        "        y_vals = np.random.uniform(low=i-density_vals, high=i+density_vals)\n",
        "      except np.linalg.LinAlgError:\n",
        "        print(f'KDE failed for feature {feature}, using scatter fallback')\n",
        "        x_vals = df_feat['SHAP Value'].values\n",
        "        y_vals = np.full_like(df_feat['SHAP Value'], fill_value=i, dtype=float)\n",
        "      val = df_feat['Feature Value'].values\n",
        "      if (val.max() - val.min()) == 0.0:\n",
        "        val_norm = np.full_like(val, fill_value=val.max(), dtype=float)\n",
        "      else:\n",
        "        val_norm = (val - val.min()) / (val.max() - val.min())\n",
        "\n",
        "      sc = ax.scatter(\n",
        "          x_vals, y_vals,\n",
        "          c=val_norm,\n",
        "          cmap='coolwarm', edgecolors='None', s=20, alpha=0.8\n",
        "      )\n",
        "\n",
        "    cbar = plt.colorbar(sc, ax=ax)\n",
        "    cbar.set_label('Feature Value')\n",
        "    cbar.set_ticks([])\n",
        "    cbar.ax.text(1.05, 0.05, 'Low', transform=cbar.ax.transAxes, va='center')\n",
        "    cbar.ax.text(1.05, 0.95, 'High', transform=cbar.ax.transAxes, va='center')\n",
        "  elif plot_type == 'bar':\n",
        "    for i, feature in enumerate(features):\n",
        "      x = df_shap[feature].values\n",
        "      y = np.full_like(x, fill_value=i, dtype=float)\n",
        "      plt.barh(y, x, align='center')\n",
        "\n",
        "  ax.axvline(0.0, c='k', linewidth=0.5)\n",
        "  ax.set_yticks(np.arange(n_feats))\n",
        "  ax.set_yticklabels(features)\n",
        "\n",
        "  ax.set_xlabel(xlabel)\n",
        "  ax.set_ylabel('Feature')\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  ax.spines['left'].set_visible(False)\n",
        "  ax.spines['right'].set_visible(False)\n",
        "\n",
        "  plt.grid(True, axis='both', linestyle='--', alpha=0.5)\n",
        "  plt.savefig(f'plots/{filename}.png', dpi=1000)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Recommendations"
      ],
      "metadata": {
        "id": "bhLN2K-wyUlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlfqO_yVfjYO"
      },
      "outputs": [],
      "source": [
        "class ShapleyAnalysis:\n",
        "  # Tools used for generating recommendations\n",
        "  def __init__(self, var_threshold, prop_thresh, pval_thresh, random_state):\n",
        "    self.var_thresh = var_threshold\n",
        "    self.prop_thresh = prop_thresh\n",
        "    self.pval_thresh = pval_thresh\n",
        "    self.random_state = random_state\n",
        "\n",
        "  def inclu_exclu_var(self, df):\n",
        "    \"\"\"\n",
        "      Generate recommendations for feature exclusion\n",
        "      Args:\n",
        "        shap_file: shap analysis values\n",
        "        sel_data: feature values\n",
        "\n",
        "      Returns:\n",
        "        The features that are non-linearly correlated with the outcome\n",
        "    \"\"\"\n",
        "    eps = np.std(df.abs().to_numpy(dtype=float), ddof=1)\n",
        "    for col in df.columns:\n",
        "      vals = df[col].values\n",
        "      ci_lo, ci_hi = self.bootstrap_analysis(vals, stats_type='mean_abs')\n",
        "      decision = (ci_lo >= -self.var_thresh*eps) and (ci_hi <= self.var_thresh*eps)\n",
        "      if decision:\n",
        "        print(col)\n",
        "\n",
        "  def non_linear_test(self, shap_file, sel_data):\n",
        "    \"\"\"\n",
        "      Generate recommendations for non-linearity\n",
        "\n",
        "      Args:\n",
        "        shap_file: shap analysis values\n",
        "        sel_data: feature values\n",
        "\n",
        "      Returns:\n",
        "        The features that are non-linearly correlated with the outcome\n",
        "    \"\"\"\n",
        "    for c in shap_file.columns:\n",
        "      corr = pearsonr(shap_file[c].to_numpy(), sel_data[c].to_numpy())\n",
        "      if np.abs(corr[0]) < 0.1:\n",
        "        print(f'{c}: {corr[0]:.2f} ({corr[1]:.2f})')\n",
        "\n",
        "  def wilcoxon_rank_sum_test(self, df1, df2):\n",
        "    \"\"\"\n",
        "      Genrate recommendations for feature interactions\n",
        "\n",
        "      Args:\n",
        "        df1, df2: shap analysis files for two populations after stratification.\n",
        "\n",
        "      Returns:\n",
        "        The features that are interacted with the stratified feature\n",
        "    \"\"\"\n",
        "    for col in df1.columns:\n",
        "        val1 = df1[col].to_numpy()\n",
        "        val2 = df2[col].to_numpy()\n",
        "\n",
        "        if len(val1) == 0 or len(val2) == 0:\n",
        "            continue\n",
        "        stat, p = mannwhitneyu(val1, val2, method='asymptotic', alternative='two-sided')\n",
        "        if p <= self.pval_thresh:\n",
        "            print(col)\n",
        "\n",
        "  def bootstrap_analysis(self, x, b=1000, alpha=0.05, stats_type='median_abs'):\n",
        "    \"\"\"\n",
        "      Bootstrap analysis for performance\n",
        "\n",
        "      Args:\n",
        "        x: dataset that will be evaluated using bootstrap analysis.\n",
        "        b: bootstrap iterations\n",
        "        alpha: confidence level\n",
        "\n",
        "      Returns:\n",
        "        low and high confidence intervals\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(self.random_state)\n",
        "    # x = x.to_numpy()\n",
        "    n = x.size\n",
        "    boot = []\n",
        "    for _ in range(b):\n",
        "        xb = resample(\n",
        "            x,\n",
        "            replace=True,\n",
        "            n_samples=n,\n",
        "            random_state=int(rng.integers(1_000_000_000)),\n",
        "        )\n",
        "        boot.append(self.get_data_stats(xb, stats_type))\n",
        "    lo, hi = np.percentile(boot, [100 * alpha / 2, 100 * (1 - alpha / 2)])\n",
        "\n",
        "    return lo, hi\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def get_data_stats(x, stats_type='mean_abs'):\n",
        "      if stats_type == 'mean_abs':\n",
        "          data = np.mean(np.abs(x), axis=0)\n",
        "      elif stats_type == 'median_abs':\n",
        "          data = np.median(np.abs(x), axis=0)\n",
        "      elif stats_type == 'mean':\n",
        "          data = np.mean(x, axis=0)\n",
        "      elif stats_type == 'median':\n",
        "          data = np.median(x, axis=0)\n",
        "      else:\n",
        "          raise ValueError('Unknown stats')\n",
        "      return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strata analysis"
      ],
      "metadata": {
        "id": "FxURJVgSzCmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84uCnmp74Nfg"
      },
      "outputs": [],
      "source": [
        "def sign_balance_test(df):\n",
        "  # Generate strata\n",
        "  result = []\n",
        "  for col in df.columns:\n",
        "    vals = df[col].values\n",
        "\n",
        "    n = len(vals[vals != 0])\n",
        "    if n < 0.1 * len(vals):\n",
        "      continue\n",
        "    n_pos = (vals > 0).sum()\n",
        "    test = binomtest(n_pos, n, 0.5)\n",
        "    balanced = (test.pvalue > 0.05)\n",
        "    if balanced:\n",
        "      print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zHHJTK5sUI5"
      },
      "outputs": [],
      "source": [
        "def strata_generate(data, variable, X_test, y_test, thresh):\n",
        "  # Split datasets into two stratas\n",
        "  mask1 = (data[variable] < thresh)\n",
        "  mask2 = (data[variable] > thresh)\n",
        "  X_test_group1 = X_test[mask1]\n",
        "  X_test_group2 = X_test[mask2]\n",
        "  y_test_group1 = y_test\n",
        "  y_test_group2 = y_test\n",
        "  X_test_list = [X_test_group1, X_test_group2]\n",
        "  y_test_list = [y_test_group1, y_test_group2]\n",
        "\n",
        "  return X_test_list, y_test_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOsbWmZVtz6m"
      },
      "outputs": [],
      "source": [
        "def stratify_shap_analysis(model, X_test_list, y_test_list, variable, risk_level):\n",
        "    num_group = len(X_test_list)\n",
        "    strata_shap = []\n",
        "    for i in range(num_group):\n",
        "        X_test = X_test_list[i]\n",
        "        y_test = y_test_list[i]\n",
        "        shap_file, sel_data = get_explanations(model, X_test, y_test, eps=0.05, risk_level=risk_level)\n",
        "        # make_plot(shap_file, sel_data, f'aids_shap_{i}_low', plot_type='violin', xlabel='SHAP value', figsize=(8, 6))\n",
        "        strata_shap.append(shap_file)\n",
        "    return strata_shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gBv1cS9yEVU"
      },
      "outputs": [],
      "source": [
        "## Generate interactions\n",
        "## run sign_balance_test to find variables to be stratified\n",
        "## strata_generate(data, variable, X_test, y_test, thresh) to generate the stratified dataset\n",
        "## Then run stratify_shap_analysis to generate stratified shapley analysis\n",
        "## Then run shapAna = ShapleyAnalysis(0.05, 0.05, 0.05, random_state=1)\n",
        "## Finally run shapAna.wilcoxcon_rank_sum_test(data1, data2) to get interactions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrate recommendations"
      ],
      "metadata": {
        "id": "UUH7jMpx1Eg2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SAbqqwLrUR9"
      },
      "outputs": [],
      "source": [
        "## Integrate feature exclusion recommendations on the dataset\n",
        "def exclusion_analysis(data, exclu_feature):\n",
        "  data = data.drop(columns=exclu_feature, errors='ignore')\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKa1NeVQ0iQP"
      },
      "outputs": [],
      "source": [
        "## Integrate non-linearity recommendations on the dataset\n",
        "def nonlinear_analysis(data, nonlinear_feature, nonlinear_type='quadratic'):\n",
        "    data_cols = data.columns\n",
        "    nonlinear_cols = []\n",
        "    for feature in nonlinear_feature:\n",
        "        if feature in data_cols:\n",
        "            name = f'{feature}_{nonlinear_type}'\n",
        "            if nonlinear_type == 'quadratic':\n",
        "                data[feature] = data[feature] - data[feature].mean()\n",
        "                quad = (data[feature]**2).rename(name)\n",
        "                nonlinear_cols.append(quad.to_frame())\n",
        "            else:\n",
        "                basis = dmatrix(f\"0+cr({feature}, df=3)\", data,\n",
        "                                return_type=\"dataframe\",\n",
        "                                NA_action='raise')\n",
        "                basis = basis.add_prefix(name)\n",
        "                basis = basis.reindex(data.index)\n",
        "                nonlinear_cols.append(basis)\n",
        "    if nonlinear_cols:\n",
        "        nonlinear = pd.concat(nonlinear_cols, axis=1)\n",
        "        nonlinear_data = pd.concat([data, nonlinear], axis=1)\n",
        "    else:\n",
        "        nonlinear_data = data.copy()\n",
        "\n",
        "    return nonlinear_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hToGYgg1rQH7"
      },
      "outputs": [],
      "source": [
        "## Integrate interaction recommendations on the dataset\n",
        "def interaction_analysis(data, inter_feat, interact_list, nonlinear_list):\n",
        "    data_cols = data.columns\n",
        "    inter_cols = {}\n",
        "    for item in interact_list:\n",
        "        a = inter_feat\n",
        "        b = item\n",
        "        if a in data_cols:\n",
        "            if b in data_cols:\n",
        "                name = f'{a}_x_{b}'\n",
        "                inter_cols[name] = data[a] * data[b]\n",
        "            if a in nonlinear_list:\n",
        "                if b in nonlinear_list:\n",
        "                    name = f'{a}_quad_x_{b}_quad'\n",
        "                    inter_cols[name] = data[a + '_quadratic'] * data[b + '_quadratic']\n",
        "                else:\n",
        "                    name = f'{a}_quad_x_{b}'\n",
        "                    inter_cols[name] = data[a + '_quadratic'] * data[b]\n",
        "            else:\n",
        "                if b in nonlinear_list:\n",
        "                    name = f'{a}_x_{b}_quad'\n",
        "                    inter_cols[name] = data[a] * data[b + '_quadratic']\n",
        "\n",
        "    inter = pd.DataFrame(inter_cols, index=data.index) if inter_cols else pd.DataFrame(index=data.index)\n",
        "    interact_data = pd.concat([data, inter], axis=1)\n",
        "\n",
        "    return interact_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgKhwIWd_vus"
      },
      "source": [
        "# Calibration\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate calibration plots"
      ],
      "metadata": {
        "id": "vIeHCGnqzWjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8mO5S7D0oOL"
      },
      "outputs": [],
      "source": [
        "class CalibrationPerform:\n",
        "  def __init__(self, t0, n_bins=5, kind='survival', n_boot=0,\n",
        "                random_state=0, save_folder=None, model_name=['RandomForest']):\n",
        "      self.t0 = t0\n",
        "      self.n_bins = n_bins\n",
        "      self.kind = kind\n",
        "      self.n_boot = n_boot\n",
        "      self.random_state = random_state\n",
        "      self.save_folder = save_folder\n",
        "      self.model_name = model_name\n",
        "\n",
        "  def _extract_time_event(self, y):\n",
        "        \"\"\"\n",
        "        Return (time, event) as 1D numpy arrays.\n",
        "\n",
        "        Supports:\n",
        "          * sksurv structured arrays from load_gbsg2, load_whas500, load_aids, ...\n",
        "          * pandas DataFrame with usual column names.\n",
        "        \"\"\"\n",
        "\n",
        "        # Case 1: structured array (what sksurv datasets return)\n",
        "        if isinstance(y, np.ndarray) and y.dtype.names is not None:\n",
        "            names = list(y.dtype.names)\n",
        "            if len(names) != 2:\n",
        "                raise ValueError(\n",
        "                    f\"Structured array y must have exactly 2 fields, got {names}\"\n",
        "                )\n",
        "\n",
        "            # By sksurv convention: first field = event indicator (bool),\n",
        "            # second field = time (float). :contentReference[oaicite:1]{index=1}\n",
        "            event = y[names[0]].astype(bool)\n",
        "            time = y[names[1]].astype(float)\n",
        "            return time, event\n",
        "\n",
        "        # Case 2: pandas DataFrame\n",
        "        if isinstance(y, pd.DataFrame):\n",
        "            # possible event column names across datasets\n",
        "            event_candidates = [\n",
        "                \"event\", \"Status\", \"status\", \"death\", \"fstat\",\n",
        "                \"cens\", \"censor\", \"censor_d\"\n",
        "            ]\n",
        "            time_candidates = [\n",
        "                \"time\", \"Survival_in_days\", \"lenfol\", \"time_d\", \"futime\"\n",
        "            ]\n",
        "\n",
        "            event_col = next((c for c in event_candidates if c in y.columns), None)\n",
        "            time_col = next((c for c in time_candidates if c in y.columns), None)\n",
        "\n",
        "            if event_col is None or time_col is None:\n",
        "                raise ValueError(\n",
        "                    \"Could not infer event/time columns in DataFrame y; \"\n",
        "                    \"got columns: \" + \", \".join(y.columns)\n",
        "                )\n",
        "\n",
        "            event = y[event_col].astype(bool).to_numpy()\n",
        "            time = y[time_col].astype(float).to_numpy()\n",
        "            return time, event\n",
        "\n",
        "        raise TypeError(\n",
        "            \"Unsupported type for y. Use a sksurv structured array or a pandas DataFrame.\"\n",
        "        )\n",
        "\n",
        "  def plot_survival_calibration(self, X, y, model):\n",
        "    rng = np.random.default_rng(self.random_state)\n",
        "    est = self.extract_survival_estimator(model)\n",
        "\n",
        "    surv_funcs = est.predict_survival_function(X, return_array=False)\n",
        "    pred_surv_t0 = np.array([float(sf(self.t0)) for sf in surv_funcs])\n",
        "\n",
        "    if self.kind == 'survival':\n",
        "        pred_cal = pred_surv_t0\n",
        "    elif self.kind == 'risk':\n",
        "        pred_cal = 1.0 - pred_surv_t0\n",
        "    else:\n",
        "        raise ValueError(\"kind must be 'survival' or 'risk'\")\n",
        "\n",
        "    edges = self.quantile_bins(pred_cal)\n",
        "    bin_idx = np.digitize(pred_cal, edges[1:-1], right=True)\n",
        "    n_bins_eff = edges.size - 1\n",
        "\n",
        "    time, event = self._extract_time_event(y)\n",
        "\n",
        "    bin_pred, bin_obs, bin_counts = [], [], []\n",
        "    obs_ci_lo, obs_ci_hi = [], []\n",
        "    MIN_N = 10\n",
        "\n",
        "    for b in range(n_bins_eff):\n",
        "        mask = bin_idx == b\n",
        "        if mask.sum() < MIN_N:\n",
        "            continue\n",
        "\n",
        "        # compute KM *first*\n",
        "        s_hat = self.km_s_at(time[mask], event[mask])\n",
        "        if np.isnan(s_hat):\n",
        "            # if KM at t0 is undefined in this bin, skip the bin entirely\n",
        "            continue\n",
        "\n",
        "        # only now append pred & obs so lengths always match\n",
        "        bin_pred.append(pred_cal[mask].mean())\n",
        "\n",
        "        obs_val = s_hat if self.kind == 'survival' else (1.0 - s_hat)\n",
        "        bin_obs.append(obs_val)\n",
        "        bin_counts.append(mask.sum())\n",
        "\n",
        "        if self.n_boot > 0:\n",
        "            boot_vals = []\n",
        "            for _ in range(self.n_boot):\n",
        "                idx = rng.integers(0, mask.sum(), mask.sum())\n",
        "                t_b = time[mask][idx]\n",
        "                e_b = event[mask][idx]\n",
        "                s_b = self.km_s_at(t_b, e_b)\n",
        "                if np.isnan(s_b):\n",
        "                    # extremely unlikely if original s_hat wasn't NaN,\n",
        "                    # but be safe and skip this resample\n",
        "                    continue\n",
        "                boot_vals.append(\n",
        "                    s_b if self.kind == 'survival' else (1.0 - s_b)\n",
        "                )\n",
        "            if len(boot_vals) > 0:\n",
        "                lo, hi = np.percentile(boot_vals, [2.5, 97.5])\n",
        "                obs_ci_lo.append(lo)\n",
        "                obs_ci_hi.append(hi)\n",
        "\n",
        "    bin_pred = np.asarray(bin_pred)\n",
        "    bin_obs = np.asarray(bin_obs)\n",
        "    bin_counts = np.asarray(bin_counts)\n",
        "    bin_obs_ci = (\n",
        "        (np.asarray(obs_ci_lo), np.asarray(obs_ci_hi))\n",
        "        if self.n_boot > 0 and len(obs_ci_lo) > 0\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"bin_pred\": bin_pred,\n",
        "        \"bin_obs\": bin_obs,\n",
        "        \"bin_counts\": bin_counts,\n",
        "        \"bin_obs_ci\": bin_obs_ci,\n",
        "        \"edges\": edges,\n",
        "        \"t0\": self.t0,\n",
        "        \"kind\": self.kind,\n",
        "    }\n",
        "\n",
        "  def calib_estimate(self, model, X, y):\n",
        "      est_res = self.plot_survival_calibration(X=X, y=y, model=model)\n",
        "      bin_pred, bin_obs, bin_n = est_res['bin_pred'], est_res['bin_obs'], est_res['bin_counts']\n",
        "\n",
        "      # coef = np.polyfit(bin_pred, bin_obs, 1)\n",
        "      # slope_ols, intercept_ols = coef[0], coef[1]\n",
        "      w = np.asarray(bin_n) if bin_n is not None else np.ones_like(bin_pred)\n",
        "      X = np.c_[np.ones_like(bin_pred), bin_pred]\n",
        "\n",
        "      # W = np.diag(w)\n",
        "      beta = np.linalg.inv(X.T @ (w[:, None] * X)) @ (X.T @ (w * bin_obs))\n",
        "      intercept, slope = beta[0], beta[1]\n",
        "\n",
        "      print(f'The intercept is {intercept}')\n",
        "      print(f'The slope is {slope}')\n",
        "\n",
        "  def calib_plot(self, model_lst, data_lst, ax=None, title=None):\n",
        "      ### Generate calibration plots\n",
        "      if ax is None:\n",
        "          fig, ax = plt.subplots(figsize=(5, 5))\n",
        "      for i, mdl in enumerate(model_lst):\n",
        "          X_i, y_i = data_lst[i][0], data_lst[i][1]\n",
        "          est_res = self.plot_survival_calibration(X=X_i, y=y_i, model=mdl)\n",
        "          bin_pred, bin_obs, bin_obs_ci = est_res['bin_pred'], est_res['bin_obs'], est_res['bin_obs_ci']\n",
        "          ax.plot(bin_pred, bin_obs, marker='o', linestyle='-',\n",
        "                  label=f'Observed (KM)_{self.model_name[i]}')\n",
        "\n",
        "          if self.kind == 'survival':\n",
        "              y_label = 'Observed survival'\n",
        "              x_label = 'Predicted survival'\n",
        "          elif self.kind == 'risk':\n",
        "              y_label = 'Observed risk'\n",
        "              x_label = 'Predicted risk'\n",
        "          else:\n",
        "              raise ValueError('Kind must be \"survival\" or \"risk\".')\n",
        "\n",
        "          minv = min(0.0, bin_pred.min(), bin_obs.min())\n",
        "          maxv = max(1.0, bin_pred.max(), bin_obs.max())\n",
        "          ax.plot([minv, maxv], [minv, maxv],\n",
        "                  color='black', linestyle='--', linewidth=1.2,\n",
        "                  label='Perfect calibration')\n",
        "          ax.set_xlim(minv, maxv)\n",
        "          ax.set_ylim(minv, maxv)\n",
        "          # ax.set_xlim(0.5, 1.0)\n",
        "          # ax.set_ylim(0.5, 1.0)\n",
        "          ax.set_xlabel(x_label)\n",
        "          ax.set_ylabel(y_label)\n",
        "          if title is None:\n",
        "              title = f'Calibration plot ({self.kind})'\n",
        "          ax.set_title(title)\n",
        "          ax.grid(True, linestyle='--', alpha=0.4)\n",
        "          ax.legend()\n",
        "\n",
        "          if bin_obs_ci is not None:\n",
        "              lo, hi = bin_obs_ci\n",
        "              ax.vlines(bin_pred, lo, hi, alpha=0.6)\n",
        "              # ax.set_xlim(0.5, 1.0)\n",
        "              # ax.set_ylim(0.5, 1.0)\n",
        "          plt.savefig(\n",
        "              os.path.join(self.save_folder,\n",
        "                          f'Calibration_plot_{self.model_name}.pdf'),\n",
        "              dpi=1000,\n",
        "              bbox_inches='tight'\n",
        "          )\n",
        "\n",
        "  def quantile_bins(self, x):\n",
        "      qs = np.linspace(0, 1, self.n_bins + 1)\n",
        "      edges = np.quantile(x, qs)\n",
        "      edges = np.unique(edges)\n",
        "\n",
        "      if edges.size < 2:\n",
        "          edges = np.array([x.min(), x.max()])\n",
        "      return edges\n",
        "\n",
        "  def km_s_at(self, time, event):\n",
        "    # Generate Kaplanâ€“Meier estimator\n",
        "    if time.size == 0:\n",
        "        return np.nan\n",
        "    t, s = kaplan_meier_estimator(event, time)\n",
        "    if self.t0 <= t.min():\n",
        "        return 1.0\n",
        "    if self.t0 > t.max():\n",
        "        return np.nan\n",
        "    eps = 1e-12\n",
        "    return float(np.interp(self.t0 - eps, t, s))\n",
        "\n",
        "  @staticmethod\n",
        "  def extract_survival_estimator(model):\n",
        "      if hasattr(model, \"predict_survival_function\"):\n",
        "          return model\n",
        "      if hasattr(model, \"named_steps\"):\n",
        "          for _, step in model.named_steps.items():\n",
        "              if hasattr(step, \"predict_survival_function\"):\n",
        "                  return step\n",
        "      raise AttributeError(\n",
        "          \"Could not find an estimator with predict_survival_function in `model`.\"\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZd_7saJFHb7"
      },
      "outputs": [],
      "source": [
        "def cox_risk_score(model, x):\n",
        "    if hasattr(model, 'decision_function'):\n",
        "        return model.decision_function(x)\n",
        "    if hasattr(model, 'predict'):\n",
        "        return model.predict(x)\n",
        "    if hasattr(model, 'predict_partial_hazard'):\n",
        "        return model.predict_partial_hazard(x).ravel()\n",
        "    raise AttributeError('Cox model has no usable scoring method')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch43pN9CJZcb"
      },
      "outputs": [],
      "source": [
        "class MetricEval:\n",
        "    def __init__(self, num_of_b, seed=1):\n",
        "        self.num_of_b = num_of_b\n",
        "        self.seed = seed\n",
        "        self.rng = np.random.default_rng(self.seed)\n",
        "\n",
        "    def boot_cindex_diff(self, y_test, s1, s2):\n",
        "        ### Generate bootstrap performance difference between two models\n",
        "        event = y_test['event'].astype(bool)\n",
        "        time = y_test['time'].astype(float)\n",
        "        n = len(time)\n",
        "\n",
        "        d_obs = self.cindex(event, time, s2) - self.cindex(event, time, s1)\n",
        "\n",
        "        diffs = np.empty(self.num_of_b)\n",
        "        for b in range(self.num_of_b):\n",
        "            idx = self.rng.integers(0, n, size=n)\n",
        "            diffs[b] = self.cindex(event[idx], time[idx], s2[idx]) - \\\n",
        "                       self.cindex(event[idx], time[idx], s1[idx])\n",
        "\n",
        "        lo, hi = np.percentile(diffs, [2.5, 97.5])\n",
        "        upper_tail = np.mean(diffs >= 0)\n",
        "        lower_tail = np.mean(diffs <= 0)\n",
        "        p_two = 2 * min(upper_tail, lower_tail)\n",
        "        p_two = min(1.0, p_two)\n",
        "\n",
        "        return d_obs, (lo, hi), p_two\n",
        "\n",
        "    def boot_matric(self, y_test, s, metric_type):\n",
        "        ### Generate bootstrap performance\n",
        "        boot_vals = np.empty(self.num_of_b)\n",
        "        if metric_type == 'c-index':\n",
        "            event = y_test['event'].astype(bool)\n",
        "            time = y_test['time'].astype(float)\n",
        "            n = len(time)\n",
        "            print(n)\n",
        "            val_obs = self.cindex(event, time, s)\n",
        "\n",
        "            for b in range(self.num_of_b):\n",
        "                idx = self.rng.integers(0, n, size=n)\n",
        "                if metric_type == 'c-index':\n",
        "                    boot_vals[b] = self.cindex(event[idx], time[idx], s[idx])\n",
        "            lo, hi = np.percentile(boot_vals, [2.5, 97.5])\n",
        "            upper_tail = np.mean(boot_vals >= val_obs)\n",
        "            lower_tail = np.mean(boot_vals <= val_obs)\n",
        "            p_two = 2 * min(upper_tail, lower_tail)\n",
        "            p_two = min(1.0, p_two)\n",
        "\n",
        "        return val_obs, (lo, hi), p_two\n",
        "\n",
        "    def cal_metric_CI(self, model, data, data_y, metric):\n",
        "        # Calculate confidence intervials for the metric\n",
        "        boot_vals = np.empty(self.num_of_b)\n",
        "        if metric == 'c-index':\n",
        "                # Make sure shapes are consistent\n",
        "            n = data.shape[0]\n",
        "\n",
        "            # If X / y are pandas, use .iloc\n",
        "            if hasattr(data, \"iloc\"):\n",
        "                get_X = lambda idx: data.iloc[idx]\n",
        "            else:\n",
        "                get_X = lambda idx: data[idx]\n",
        "\n",
        "            if hasattr(y, \"iloc\"):\n",
        "                get_y = lambda idx: data_y.iloc[idx]\n",
        "            else:\n",
        "                get_y = lambda idx: data_y[idx]\n",
        "\n",
        "            val_obs = model.score(data, data_y)\n",
        "\n",
        "            boot_vals = np.empty(self.num_of_b)\n",
        "            for b in range(self.num_of_b):\n",
        "                idx = self.rng.integers(0, n, size=n)\n",
        "                X_b = get_X(idx)\n",
        "                y_b = get_y(idx)\n",
        "                if metric == 'c-index':\n",
        "                    boot_vals[b] = model.score(data, data_y)\n",
        "            lo, hi = np.percentile(boot_vals, [2.5, 97.5])\n",
        "            upper_tail = np.mean(boot_vals >= val_obs)\n",
        "            lower_tail = np.mean(boot_vals <= val_obs)\n",
        "            p_two = 2 * min(upper_tail, lower_tail)\n",
        "            p_two = min(1.0, p_two)\n",
        "        print(f'{metric} = {val_obs:.3f} (95% CI {lo:.3f}-{hi:.3f}), p = {p_two:.4f}')\n",
        "\n",
        "    @staticmethod\n",
        "    def cindex(event, time, s):\n",
        "        c, *_ = concordance_index_censored(event, time, s)\n",
        "        return c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDgVxfMy_4Ji"
      },
      "source": [
        "# GBSG 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Baseline and the Recommendar"
      ],
      "metadata": {
        "id": "K9X_b6tw2cob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hG-l3B92Qlp"
      },
      "outputs": [],
      "source": [
        "## Load the dataset and train the naive model and the recommendar.\n",
        "X, y = load_gbsg2()\n",
        "\n",
        "grade_str = X.loc[:, \"tgrade\"].astype(object).values[:, np.newaxis]\n",
        "grade_num = OrdinalEncoder(categories=[[\"I\", \"II\", \"III\"]]).fit_transform(grade_str)\n",
        "\n",
        "X_no_grade = X.drop(\"tgrade\", axis=1)\n",
        "Xt = OneHotEncoder().fit_transform(X_no_grade)\n",
        "Xt.loc[:, \"tgrade\"] = grade_num\n",
        "con_cols = ['age', 'estrec', 'progrec', 'tsize']\n",
        "Xt[con_cols] = StandardScaler().fit_transform(Xt[con_cols])\n",
        "\n",
        "Xt.rename(columns={'horTh=yes': 'horTh', 'menostat=Post': 'menostat'}, inplace=True)\n",
        "\n",
        "ex_model, (X_train, y_train), (X_test, y_test) = split_and_train(Xt, y, 'rf')\n",
        "org_model, (X_train, y_train), (X_test, y_test) = split_and_train(Xt, y, 'cox')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MBiaDnMF5fH"
      },
      "outputs": [],
      "source": [
        "## Evaluate original model\n",
        "evaluator = MetricEval(1000)\n",
        "# evaluator.cal_metric_CI(org_model, X_test, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=1500, n_bins=5, kind='survival', random_state=0, save_folder='plots/', model_name=['Cox_org'])\n",
        "calib.calib_plot([org_model], [[X_test, y_test]])\n",
        "calib.calib_estimate(org_model, X_test, y_test)\n",
        "\n",
        "## Evaluate rsf model\n",
        "evaluator = MetricEval(1000)\n",
        "# evaluator.cal_metric_CI(ex_model, X_test, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=1500, n_bins=5, kind='survival', random_state=0, save_folder='plots/', model_name=['rsf'])\n",
        "calib.calib_plot([ex_model], [[X_test, y_test]])\n",
        "calib.calib_estimate(ex_model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate recommendations"
      ],
      "metadata": {
        "id": "YdK7cXCn2X9N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OyfPfHHmomT"
      },
      "outputs": [],
      "source": [
        "## Get feature attribution\n",
        "org_X = X_train.copy()\n",
        "org_y = y_train.copy()\n",
        "df_shap_low, sel_data_low = get_explanations(ex_model, org_X, org_y, eps=0.2, risk_level='low')\n",
        "make_plot(df_shap_low, sel_data_low, 'GBSG2_org_shap_low', plot_type='violin', xlabel='SHAP value', figsize=(8, 6))\n",
        "df_shap_high, sel_data_high = get_explanations(ex_model, org_X, org_y, eps=0.2, risk_level='high')\n",
        "make_plot(df_shap_high, sel_data_high, 'GBSG2_org_shap_high', plot_type='violin', xlabel='SHAP value', figsize=(8, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38WV0wLbAcX9"
      },
      "outputs": [],
      "source": [
        "shapana = ShapleyAnalysis(0.05, 0.05, 0.05, random_state=20)\n",
        "shapana.inclu_exclu_var(df_shap_low)\n",
        "shapana.inclu_exclu_var(df_shap_high)\n",
        "print('======non-linear======')\n",
        "shapana.non_linear_test(df_shap_low, sel_data_low)\n",
        "shapana.non_linear_test(df_shap_high, sel_data_high)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-mZFLnb3Mlq"
      },
      "outputs": [],
      "source": [
        "print('====== Low risk cohort ======')\n",
        "sign_balance_test(df_shap_low)\n",
        "print('====== High risk cohort ======')\n",
        "sign_balance_test(df_shap_high)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vspJSa5VBJuZ"
      },
      "outputs": [],
      "source": [
        "strata = {'age':0, 'estrec':0, 'progrec':6, 'horTh':-2}\n",
        "# strata = {'pnodes':-1, 'tsize':0}\n",
        "for variable, thresh in strata.items():\n",
        "  print(f'The stratified variable is {variable}')\n",
        "  X_test_list, y_test_list =strata_generate(df_shap_low, variable, sel_data_low.reset_index(drop=True), y_test, thresh)\n",
        "  shap_file = stratify_shap_analysis(ex_model, X_test_list, y_test_list, variable, risk_level=None)\n",
        "  df1, df2 = shap_file[0], shap_file[1]\n",
        "  shapana.wilcoxon_rank_sum_test(df1, df2)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbVgbWwWcxZw"
      },
      "outputs": [],
      "source": [
        "strata = {'age':0, 'tsize':0, 'tgrade':0, 'horTh':-1, 'estrec':0, 'progrec':20}\n",
        "\n",
        "for variable, thresh in strata.items():\n",
        "  print(f'The stratified variable is {variable}')\n",
        "  X_test_list, y_test_list =strata_generate(df_shap_high, variable, sel_data_high.reset_index(drop=True), y_test, thresh)\n",
        "  shap_file = stratify_shap_analysis(ex_model, X_test_list, y_test_list, variable, risk_level=None)\n",
        "  df1, df2 = shap_file[0], shap_file[1]\n",
        "  shapana.wilcoxon_rank_sum_test(df1, df2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrate recommendations"
      ],
      "metadata": {
        "id": "fgm3UZRb2LGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHw0WnnVdxAs"
      },
      "outputs": [],
      "source": [
        "nonlinear_feature = ['age']\n",
        "nonlinear_train = X_train.copy()\n",
        "nonlinear_test = X_test.copy()\n",
        "X_train_nonlinear = nonlinear_analysis(nonlinear_train, nonlinear_feature, nonlinear_type='quadratic')\n",
        "X_test_nonlinear = nonlinear_analysis(nonlinear_test, nonlinear_feature, nonlinear_type='quadratic')\n",
        "cox_new_model = get_model('cox', 20)\n",
        "cox_new_model.fit(X_train_nonlinear, y_train)\n",
        "\n",
        "evaluator.cal_metric_CI(cox_new_model, X_test_nonlinear, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=1500, kind='survival', random_state=0, save_folder='plots/', model_name=['Cox_org','Cox_nonlinear'])\n",
        "calib.calib_plot([org_model, cox_new_model], [[X_test, y_test], [X_test_nonlinear,y_test]])\n",
        "calib.calib_estimate(cox_new_model, X_test_nonlinear, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoyUWR9jTcxl"
      },
      "outputs": [],
      "source": [
        "inter_feat_total = ['age','estrec','progrec', 'horTh']\n",
        "interaction_list_total = [['tsize'], ['pnodes', 'age'],\n",
        " ['age', 'pnodes', 'estrec', 'horTh', 'tgrade', 'menostat'],['menostat','tsize', 'tgrade','pnodes', 'age']]\n",
        "non_linear_list = []\n",
        "# non_linear_list = ['age', 'tsize']\n",
        "X_test_interact = X_test.copy()\n",
        "X_train_interact = X_train.copy()\n",
        "for i in range(len(inter_feat_total)):\n",
        "  inter_feat = inter_feat_total[i]\n",
        "  interaction_list = interaction_list_total[i]\n",
        "  X_train_interact = interaction_analysis(X_train_interact,inter_feat, interaction_list, non_linear_list)\n",
        "  X_test_interact = interaction_analysis(X_test_interact,inter_feat, interaction_list, non_linear_list)\n",
        "\n",
        "cox_new_model = get_model('cox', 20)\n",
        "cox_new_model.fit(X_train_interact, y_train)\n",
        "\n",
        "evaluator.cal_metric_CI(cox_new_model, X_test_interact, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=1500, kind='survival', random_state=0, save_folder='plots/', model_name=['Cox_org','Cox_inter'])\n",
        "calib.calib_plot([org_model, cox_new_model], [[X_test, y_test], [X_test_interact,y_test]])\n",
        "calib.calib_estimate(cox_new_model, X_test_interact, y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcwncvt_5Bq2"
      },
      "outputs": [],
      "source": [
        "inter_feat_total = ['age','estrec','progrec', 'horTh']\n",
        "interaction_list_total = [['tsize'], ['pnodes', 'age'],\n",
        " ['age', 'pnodes', 'estrec', 'horTh', 'tgrade', 'menostat'],['menostat','tsize', 'tgrade','pnodes', 'age']]\n",
        "non_linear_list = ['age']\n",
        "X_test_interact = X_test_nonlinear.copy()\n",
        "X_train_interact = X_train_nonlinear.copy()\n",
        "for i in range(len(inter_feat_total)):\n",
        "  inter_feat = inter_feat_total[i]\n",
        "  interaction_list = interaction_list_total[i]\n",
        "  X_train_interact = interaction_analysis(X_train_interact,inter_feat, interaction_list, non_linear_list)\n",
        "  X_test_interact = interaction_analysis(X_test_interact,inter_feat, interaction_list, non_linear_list)\n",
        "\n",
        "cox_new_model = get_model('cox', 20)\n",
        "cox_new_model.fit(X_train_interact, y_train)\n",
        "\n",
        "evaluator.cal_metric_CI(cox_new_model, X_test_interact, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=1500, kind='survival', random_state=0, save_folder='plots/', model_name=['Cox_org','Cox_all'])\n",
        "calib.calib_plot([org_model, cox_new_model], [[X_test, y_test], [X_test_interact,y_test]])\n",
        "calib.calib_estimate(cox_new_model, X_test_interact, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xJ6MfKcrVR3"
      },
      "source": [
        "# ACT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur_8__PQrUuM"
      },
      "outputs": [],
      "source": [
        "X, y = load_whas500()\n",
        "\n",
        "cat_cols = ['afb', 'av3', 'chf', 'cvd', 'gender', 'miord', 'mitype', 'sho']\n",
        "X[cat_cols] = OrdinalEncoder().fit_transform(X[cat_cols])\n",
        "\n",
        "ex_model, (X_train, y_train), (X_test, y_test) = split_and_train(X, y, 'rf')\n",
        "org_model, (X_train, y_train), (X_test, y_test) = split_and_train(X, y, 'cox')\n",
        "\n",
        "con_cols = ['age', 'bmi', 'diasbp', 'hr', 'sysbp']\n",
        "X[con_cols] = StandardScaler().fit_transform(X[con_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33EjQphDzlgw"
      },
      "outputs": [],
      "source": [
        "## Evaluate original model\n",
        "evaluator = MetricEval(1000)\n",
        "evaluator.cal_metric_CI(org_model, X_test, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=320, kind='survival', random_state=0, save_folder='plots/act/', model_name=['Cox_org'])\n",
        "calib.calib_plot([org_model], [[X_test, y_test]])\n",
        "calib.calib_estimate(org_model, X_test, y_test)\n",
        "\n",
        "## Evaluate rsf model\n",
        "evaluator = MetricEval(1000)\n",
        "evaluator.cal_metric_CI(ex_model, X_test, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=320, kind='survival', random_state=0, save_folder='plots/act/', model_name=['rsf'])\n",
        "calib.calib_plot([ex_model], [[X_test, y_test]])\n",
        "calib.calib_estimate(ex_model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-xaEgBx2GDi"
      },
      "outputs": [],
      "source": [
        "## Get feature interaction\n",
        "org_X = X_train.copy()\n",
        "org_y = y_train.copy()\n",
        "df_shap_low, sel_data_low = get_explanations(ex_model, org_X, org_y, eps=0.1, risk_level='low')\n",
        "make_plot(df_shap_low, sel_data_low, 'whas_org_shap_low_train', plot_type='violin', xlabel='SHAP value', figsize=(8, 6))\n",
        "df_shap_high, sel_data_high = get_explanations(ex_model, org_X, org_y, eps=0.1, risk_level='high')\n",
        "make_plot(df_shap_high, sel_data_high, 'whas_org_shap_high_train', plot_type='violin', xlabel='SHAP value', figsize=(8, 6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REmToWwT477c"
      },
      "outputs": [],
      "source": [
        "shapana = ShapleyAnalysis(0.05, 0.05, 0.05, random_state=20)\n",
        "shapana.inclu_exclu_var(df_shap_low)\n",
        "shapana.inclu_exclu_var(df_shap_high)\n",
        "print('========non-linear=======')\n",
        "shapana.non_linear_test(df_shap_low, sel_data_low)\n",
        "shapana.non_linear_test(df_shap_high, sel_data_high)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_t2KjOM5RUZ"
      },
      "outputs": [],
      "source": [
        "exclu_cols = ['av3']\n",
        "exclu_train = X_train.copy()\n",
        "exclu_test = X_test.copy()\n",
        "X_train_exclu = exclusion_analysis(exclu_train, exclu_cols)\n",
        "X_test_exclu = exclusion_analysis(exclu_test, exclu_cols)\n",
        "cox_new_model = get_model('cox', 20)\n",
        "cox_new_model.fit(X_train_exclu, y_train)\n",
        "\n",
        "evaluator.cal_metric_CI(cox_new_model, X_test_exclu, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=200, kind='survival', random_state=0, save_folder='plots/act/', model_name=['Cox_org','Cox_exclu'])\n",
        "calib.calib_plot([org_model, cox_new_model], [[X_test, y_test], [X_test_exclu,y_test]])\n",
        "calib.calib_estimate(cox_new_model, X_test_exclu, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykA_4cKp-Zix"
      },
      "outputs": [],
      "source": [
        "nonlinear_feature = ['sysbp']\n",
        "nonlinear_train = X_train.copy()\n",
        "nonlinear_test = X_test.copy()\n",
        "X_train_nonlinear = nonlinear_analysis(nonlinear_train, nonlinear_feature, nonlinear_type='quadratic')\n",
        "X_test_nonlinear = nonlinear_analysis(nonlinear_test, nonlinear_feature, nonlinear_type='quadratic')\n",
        "cox_new_model = get_model('cox', 20)\n",
        "cox_new_model.fit(X_train_nonlinear, y_train)\n",
        "\n",
        "evaluator.cal_metric_CI(cox_new_model, X_test_nonlinear, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=320, kind='survival', random_state=0, save_folder='plots/act/', model_name=['Cox_org','Cox_nonlinear'])\n",
        "calib.calib_plot([org_model, cox_new_model], [[X_test, y_test], [X_test_nonlinear,y_test]])\n",
        "calib.calib_estimate(cox_new_model, X_test_nonlinear, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('====== Low risk cohort ======')\n",
        "sign_balance_test(df_shap_low)\n",
        "print('====== High risk cohort ======')\n",
        "sign_balance_test(df_shap_high)"
      ],
      "metadata": {
        "id": "He2urYJofKBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GNZ9Oyi_p5P"
      },
      "outputs": [],
      "source": [
        "strata = {'bmi':0, 'cvd':0, 'diasbp':0, 'gender':1, 'sysbp':1, 'age':-0.5, 'mitype':0,'hr':-2}\n",
        "for variable, thresh in strata.items():\n",
        "  print(f'The stratified variable is {variable}')\n",
        "  X_test_list, y_test_list =strata_generate(df_shap_low, variable, sel_data_low.reset_index(drop=True), y_test, thresh)\n",
        "  shap_file = stratify_shap_analysis(ex_model, X_test_list, y_test_list, variable, risk_level=None)\n",
        "  df1, df2 = shap_file[0], shap_file[1]\n",
        "  shapana.wilcoxon_rank_sum_test(df1, df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db_A_wq7_aVw"
      },
      "outputs": [],
      "source": [
        "inter_feat_total = ['cvd','diasbp', 'gender', 'sysbp','age','mitype','hr']\n",
        "interaction_list_total = [['diasbp', 'miord'],['bmi','sysbp'],['los','sysbp'],['mitype'],\n",
        "                          ['bmi','gender','los','sysbp'],['miord'],['bmi']]\n",
        "# non_linear_list = ['karnof', 'age']\n",
        "non_linear_list = []\n",
        "X_test_interact = X_test.copy()\n",
        "X_train_interact = X_train.copy()\n",
        "for i in range(len(inter_feat_total)):\n",
        "  inter_feat = inter_feat_total[i]\n",
        "  interaction_list = interaction_list_total[i]\n",
        "  X_train_interact = interaction_analysis(X_train_interact,inter_feat, interaction_list, non_linear_list)\n",
        "  X_test_interact = interaction_analysis(X_test_interact,inter_feat, interaction_list, non_linear_list)\n",
        "\n",
        "cox_new_model = get_model('cox', 20)\n",
        "cox_new_model.fit(X_train_interact, y_train)\n",
        "\n",
        "evaluator.cal_metric_CI(cox_new_model, X_test_interact, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=320, kind='survival', random_state=0, save_folder='plots/act/', model_name=['Cox_org','Cox_inter'])\n",
        "calib.calib_plot([org_model, cox_new_model], [[X_test, y_test], [X_test_interact,y_test]])\n",
        "calib.calib_estimate(cox_new_model, X_test_interact, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_final = X_train_exclu.copy()\n",
        "X_test_final = X_test_exclu.copy()\n",
        "\n",
        "X_train_final = nonlinear_analysis(X_train_final, nonlinear_feature, nonlinear_type='quadratic')\n",
        "X_test_final = nonlinear_analysis(X_test_final, nonlinear_feature, nonlinear_type='quadratic')\n",
        "\n",
        "inter_feat_total = ['cvd','diasbp', 'gender', 'sysbp','age','mitype','hr']\n",
        "interaction_list_total = [['diasbp', 'miord'],['bmi','sysbp'],['los','sysbp'],['mitype'],\n",
        "                          ['bmi','gender','los','sysbp'],['miord'],['bmi']]\n",
        "non_linear_list = ['sysbp']\n",
        "\n",
        "X_test_interact = X_test_final.copy()\n",
        "X_train_interact = X_train_final.copy()\n",
        "for i in range(len(inter_feat_total)):\n",
        "  inter_feat = inter_feat_total[i]\n",
        "  interaction_list = interaction_list_total[i]\n",
        "  X_train_interact = interaction_analysis(X_train_interact,inter_feat, interaction_list, non_linear_list)\n",
        "  X_test_interact = interaction_analysis(X_test_interact,inter_feat, interaction_list, non_linear_list)\n",
        "cox_new_model = get_model('cox', 20)\n",
        "cox_new_model.fit(X_train_interact, y_train)\n",
        "\n",
        "evaluator.cal_metric_CI(cox_new_model, X_test_interact, y_test,'c-index')\n",
        "calib = CalibrationPerform(t0=320, kind='survival', random_state=0, save_folder='plots/act/', model_name=['Cox_org','Cox_all'])\n",
        "calib.calib_plot([org_model, cox_new_model], [[X_test, y_test], [X_test_interact,y_test]])\n",
        "calib.calib_estimate(cox_new_model, X_test_interact, y_test)"
      ],
      "metadata": {
        "id": "Xzt6yyqNuAGU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pM5pLkzXi4tP"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}